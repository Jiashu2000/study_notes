{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5498b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b5f466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable arrow-based columnar data transfer\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77dcbce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea7a21c",
   "metadata": {},
   "source": [
    "## Converting to/from Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67ec91c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n",
      "[Stage 0:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Dataframe Result Statistics: \n",
      "                0           1           2\n",
      "count  100.000000  100.000000  100.000000\n",
      "mean     0.552710    0.494376    0.546048\n",
      "std      0.277863    0.311639    0.303372\n",
      "min      0.024243    0.002103    0.004011\n",
      "25%      0.287379    0.227527    0.287459\n",
      "50%      0.621190    0.502328    0.593260\n",
      "75%      0.745210    0.793043    0.808989\n",
      "max      0.996406    0.989879    0.999916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# generate a pandas dataframe\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# create a spark dataframe from a pandas dataframe using arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# convert the spark dataframe back to a pandas dataframe using arrow\n",
    "result_pdf = df.select(\"*\").toPandas()\n",
    "\n",
    "print(\"Pandas Dataframe Result Statistics: \\n%s\\n\" %str(result_pdf.describe()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd690bb9",
   "metadata": {},
   "source": [
    "## Pandas UDFs (Vectorized UDFs)\n",
    "\n",
    "Pandas UDFs are user defined functions that are executed by spark using arrow to transfer data and pandas to work with the data, which allows vectorized operations.\n",
    "\n",
    "Internally, pyspark will execute a pandas UDF by splitting columns into batches and calling the function for each batch as a subset of the data, then concatenating the results together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cc5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5075b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"col1 string, col2 string\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3505b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [[1, 'a string', ('a nested string', )]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7615a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97d31663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func('long_col', 'string_col', 'struct_col')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f75770",
   "metadata": {},
   "source": [
    "### Series to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a1e1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea50f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return  a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73779fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply = pandas_udf(multiply_func, returnType = LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d0e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# a function for pandas_udf should be able to execute with local pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a301686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(series.dtype):\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(x, columns = ['x']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33a785bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(multiply(col(\"x\"), col('x'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922db87",
   "metadata": {},
   "source": [
    "### Iterator of Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b9516ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bff97f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns = ['x'])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13efd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('long')\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for x in iterator:\n",
    "        yield x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8e04215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "df.select(plus_one(col('x'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9beee35",
   "metadata": {},
   "source": [
    "### Iterator of Multiple Series to Iterator of Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d228335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "984af52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame([1, 2, 3], columns = ['x'])\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71288a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def multiply_two_cols(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    for a, b in iterator:\n",
    "        yield a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "861115ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|multiply_two_cols(x, x)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      4|\n",
      "|                      9|\n",
      "+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "df.select(multiply_two_cols(\"x\", \"x\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc208d5",
   "metadata": {},
   "source": [
    "### Series to Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a144145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27b8c7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "    (\"id\", \"v\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eb00a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"double\")\n",
    "def mean_udf(v : pd.Series) -> float:\n",
    "    return v.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77222f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                        (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|mean_udf(v)|\n",
      "+-----------+\n",
      "|        4.2|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(mean_udf(col('v'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7f1ca7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|mean_udf(v)|\n",
      "+---+-----------+\n",
      "|  1|        1.5|\n",
      "|  2|        6.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"id\").agg(mean_udf(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6211009",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window \\\n",
    "    .partitionBy('id') \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3d9e4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|   v|mean_v|\n",
      "+---+----+------+\n",
      "|  1| 1.0|   1.5|\n",
      "|  1| 2.0|   1.5|\n",
      "|  2| 3.0|   6.0|\n",
      "|  2| 5.0|   6.0|\n",
      "|  2|10.0|   6.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"mean_v\", mean_udf(df['v']).over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef82076",
   "metadata": {},
   "source": [
    "## Pandas Function APIs\n",
    "\n",
    "Pandas Function APIs can directly apply a Python native function against the whole DataFrame by using Pandas instances. Internally it works similarly with Pandas UDFs by using Arrow to transfer data and Pandas to work with the data, which allows vectorized operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651d9b1",
   "metadata": {},
   "source": [
    "### Grouped Map\n",
    "\n",
    "Grouped map operations with Pandas instances are supported by DataFrame.groupby().applyInPandas() which requires a Python function that takes a pandas.DataFrame and return another pandas.DataFrame. It maps each group to each pandas.DataFrame in the Python function.\n",
    "\n",
    "This API implements the “split-apply-combine” pattern which consists of three steps:\n",
    "\n",
    "- Split the data into groups by using DataFrame.groupBy().\n",
    "- Apply a function on each group. The input and output of the function are both pandas.DataFrame. The input data contains all the rows and columns for each group.\n",
    "- Combine the results into a new PySpark DataFrame.\n",
    "\n",
    "To use DataFrame.groupBy().applyInPandas(), the user needs to define the following:\n",
    "\n",
    "- A Python function that defines the computation for each group.\n",
    "- A StructType object or a string that defines the schema of the output PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a769f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "[(1,1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "(\"id\", 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54ee6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_mean(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v = v - v.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54e1e391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|   v|\n",
      "+---+----+\n",
      "|  1|-0.5|\n",
      "|  1| 0.5|\n",
      "|  2|-3.0|\n",
      "|  2|-1.0|\n",
      "|  2| 4.0|\n",
      "+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "df.groupby('id').applyInPandas(subtract_mean, schema = \"id long, v double\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b2ff2d",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "Map operations with Pandas instances are supported by DataFrame.mapInPandas() which maps an iterator of pandas.DataFrames to another iterator of pandas.DataFrames that represents the current PySpark DataFrame and returns the result as a PySpark DataFrame. The function takes and outputs an iterator of pandas.DataFrame. It can return the output of arbitrary length in contrast to some Pandas UDFs although internally it works similarly with Series to Series Pandas UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ce708479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df1572ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, 21), (2, 30)], ('id', 'age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9244d0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_func(iterator: Iterable[pd.DataFrame]) -> Iterable[pd.DataFrame]:\n",
    "    for pdf in iterator:\n",
    "        yield pdf[pdf.id == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e4fdeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  1| 21|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.mapInPandas(filter_func, schema = df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7385c1",
   "metadata": {},
   "source": [
    "### Co-grouped Map\n",
    "\n",
    "Co-grouped map operations with Pandas instances are supported by DataFrame.groupby().cogroup().applyInPandas() which allows two PySpark DataFrames to be cogrouped by a common key and then a Python function applied to each cogroup. It consists of the following steps:\n",
    "\n",
    "- Shuffle the data such that the groups of each dataframe which share a key are cogrouped together.\n",
    "- Apply a function to each cogroup. The input of the function is two pandas.DataFrame (with an optional tuple representing the key). The output of the function is a pandas.DataFrame.\n",
    "- Combine the pandas.DataFrames from all groups into a new PySpark DataFrame.\n",
    "\n",
    "To use groupBy().cogroup().applyInPandas(), the user needs to define the following:\n",
    "\n",
    "- A Python function that defines the computation for each cogroup.\n",
    "- A StructType object or a string that defines the schema of the output PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "969e8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    (\"time\", \"id\", \"v1\"))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
    "    (\"time\", \"id\", \"v2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "359a5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ordered(left: pd.DataFrame, right: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.merge_ordered(left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0068982d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+----+\n",
      "|    time| id| v1|  v2|\n",
      "+--------+---+---+----+\n",
      "|20000101|  1|1.0|   x|\n",
      "|20000102|  1|3.0|NULL|\n",
      "|20000101|  2|2.0|   y|\n",
      "|20000102|  2|4.0|NULL|\n",
      "+--------+---+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py:224: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n"
     ]
    }
   ],
   "source": [
    "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
    "    merge_ordered, schema = \"time int, id int, v1 double, v2 string\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85739e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
